
# Курс «Продвинутая видеоаналитика»

## Лекция 1. Видео как сигнал: от пикселей к битам

---

### 1. Введение: видео как трёхмерный сигнал

Видео — это не просто набор изображений, а **дискретная аппроксимация непрерывного оптического поля**.
Каждый кадр фиксирует распределение яркости сцены в момент времени, а вся последовательность образует функцию:
[
I(x, y, t) \rightarrow \text{яркость в точке } (x, y) \text{ в момент } t.
]
При съёмке сцена дискретизируется по трём осям:
[
x_m = m\Delta x,\quad y_n = n\Delta y,\quad t_k = k\Delta t,
]
где (\Delta x, \Delta y) — шаги по пространству, а (\Delta t = 1/f) — шаг по времени, обратно пропорциональный частоте кадров (f).

Таким образом, видеопоток — это **дискретная выборка непрерывного сигнала**.
Любое несоблюдение частоты дискретизации ведёт к искажению — **алиасингу (aliasing)**, при котором высокочастотные компоненты выглядят как ложные детали или «рывки» движения.
Максимальная частота, воспроизводимая без искажений, ограничена половиной частоты дискретизации — это **критерий Найквиста**:
[
f_{max} < \frac{f_{sampling}}{2}.
]
Для видео это означает: если движение содержит частоты выше половины FPS, оно будет восприниматься неправильно — появится смазывание или дрожание кадров.

---

### 2. Экспозиция, частота кадров и плавность движения

Каждый кадр формируется за время, пока затвор камеры открыт — **время экспозиции (shutter time)**.
Интенсивность пикселя — это интеграл по времени:
[
I(x, y) = \frac{1}{T}\int_{t_0}^{t_0 + T} L(x, y, t), dt,
]
где (L) — мгновенная яркость сцены, (T) — длительность затвора.
При длинной экспозиции кадр содержит усреднённое движение (motion blur), при короткой — «резкие», но потенциально рваные кадры.
Оптимум выбирается балансом между плавностью и соотношением сигнал/шум (SNR).

Например, при 30 FPS и затворе 1/60 с кадры перекрываются во времени — именно это создаёт визуальную плавность.

---

### 3. CFR, VFR и временные метки (PTS, DTS)

Видеофайлы бывают двух типов по синхронизации кадров:

* **CFR (Constant Frame Rate)** — постоянная частота кадров, интервалы между кадрами равны.
* **VFR (Variable Frame Rate)** — интервалы различны, каждому кадру присвоено собственное время.

Для каждого кадра фиксируются два ключевых параметра:

* **PTS (Presentation Timestamp)** — момент отображения кадра;
* **DTS (Decoding Timestamp)** — момент, когда кадр должен быть декодирован.

Для кадров типа B (bidirectional) (DTS < PTS), поскольку им нужны будущие кадры для декодирования.
Важна также величина **time_base**, задающая единицу времени внутри контейнера (для MPEG — (1/90000) с).
Корректное время кадра вычисляется по формуле:
[
t = PTS \times time_base.
]

Ошибки в этих вычислениях вызывают **дрейф времени** и рассинхронизацию звука и изображения.
В инженерных пайплайнах FFmpeg/AVFoundation такие рассчёты должны выполняться строго в плавающих секундах, а не через целочисленные деления.

---

### 4. I, P, B кадры и структура GOP

Видео состоит из **групп картинок (GOP — Group of Pictures)**.
Основные типы кадров:

* **I-frame (Intra)** — ключевой, содержит полную информацию о кадре.
* **P-frame (Predicted)** — хранит разницу относительно предыдущего.
* **B-frame (Bidirectional)** — интерполируется между соседними I/P кадрами.

Структура может выглядеть так:
[
I,P,B,B,P,B,B,I,\dots
]
Такая организация позволяет существенно уменьшить размер файла: хранится не каждый кадр, а лишь ключевые и приращения.
Однако это создаёт **задержку GOP delay** — видео нельзя начать воспроизводить, пока не декодирован ближайший ключевой кадр.

Для видеоаналитики в реальном времени GOP обычно делают коротким (8–12 кадров) или применяют **All-Intra режим** — все кадры ключевые, без межкадровых зависимостей.

---

### 5. Компрессия и кодеки

**Компрессия видео** устраняет пространственную и временную избыточность.
Используются два типа сжатия:

1. **Внутрикадровое (Intra-frame)** — аналог JPEG: блоки 8×8 или 16×16 обрабатываются через дискретное косинусное преобразование (DCT):
   [
   F(u,v) = \sum_{x,y} f(x,y)\cos!\left[\frac{(2x+1)u\pi}{2N}\right]\cos!\left[\frac{(2y+1)v\pi}{2N}\right].
   ]
2. **Межкадровое (Inter-frame)** — кодируется разница между кадрами с учётом движения (motion vectors).

Качество и битрейт регулируются функцией **rate–distortion**:
[
J = R + \lambda D, \quad D = \text{MSE или PSNR},
]
где (R) — скорость потока, (D) — искажение, (\lambda) — параметр баланса.
Параметр **CRF (Constant Rate Factor)** задаёт этот компромисс:
CRF ≈ 18 – 23 — хорошее качество, > 28 — заметные потери.

#### Основные кодеки:

| Кодек            | Описание                                  | Особенности                             |
| ---------------- | ----------------------------------------- | --------------------------------------- |
| **H.264 (AVC)**  | промышленный стандарт                     | компрессия ~1:50, поддерживается везде  |
| **H.265 (HEVC)** | преемник H.264                            | +30–40 % эффективности, но CPU-дорог    |
| **VP9**          | открытый от Google                        | используется на YouTube                 |
| **AV1**          | открытый стандарт Alliance for Open Media | до 50 % лучше H.264, активно внедряется |
| **H.266 (VVC)**  | для 8K и HDR                              | сжатие до 100×, требует спец-железа     |

---

### 6. Контейнеры и их отличие от кодеков

**Контейнер (MP4, MKV, MOV, AVI, TS)** — это «обёртка» файла, где хранятся:

* потоки видео, аудио, субтитров;
* временные метки и метаданные.

**Кодек** — алгоритм кодирования и декодирования потока.
Один и тот же контейнер может содержать разные кодеки (например, MP4 → H.264 или AV1).
Ошибки воспроизведения «файл открывается, но не играет» чаще всего вызваны не несовместимостью контейнера, а отсутствием нужного декодера.

---

### 7. Цветовые пространства: от RGB к YCbCr

RGB удобно для визуализации, но неэффективно для сжатия: три канала сильно коррелированы.
Для кодирования используют **яркостно-цветоразностное представление**:
[
\begin{bmatrix}
Y \ C_b \ C_r
\end{bmatrix}
=============

\begin{bmatrix}
0.2126 & 0.7152 & 0.0722 \
-0.1146 & -0.3854 & 0.5000 \
0.5000 & -0.4542 & -0.0458
\end{bmatrix}
\begin{bmatrix}
R \ G \ B
\end{bmatrix}
]
(матрица BT.709 для HDTV).

Y — яркость (luma), Cb/Cr — цветовые отклонения.
Глаз чувствителен к яркости сильнее, чем к цвету, поэтому цвет можно хранить с меньшим разрешением.

---

### 8. Хромасубсемплинг

Для экономии данных используют разные схемы хранения хромы:

| Схема     | Соотношение яркость:цвет      | Комментарий             |
| --------- | ----------------------------- | ----------------------- |
| **4:4:4** | Без потерь, полный цвет       | максимум качества       |
| **4:2:2** | Цвет понижен по горизонтали   | профессиональное видео  |
| **4:2:0** | Цвет вдвое реже по обеим осям | стандарт для трансляций |

В 4:2:0 на 4 пикселя хранится одна пара Cb/Cr.
Для человеческого глаза различие минимально, но для CV-моделей это может вносить артефакты на границах объектов.
Поэтому при подготовке датасетов рекомендуется использовать **4:4:4** или выполнять **upsampling** хромы при декодировании.

---

### 9. Гамма-коррекция и битовая глубина

Сенсор камеры измеряет интенсивность линейно, но человеческое зрение нелинейно.
Для согласования вводится **гамма-коррекция**:
[
V_{out} = V_{in}^{1/\gamma}, \quad \gamma \approx 2.2.
]
Это перераспределяет уровни яркости — больше градаций в тенях, меньше в светах.
При работе с нейросетями данные часто нужно **разгаммировать**, т. е. вернуть линейное пространство.

Количество уровней яркости определяется глубиной квантования:
[
N = 2^b,
]
где (b) — число бит.
8-битное видео → 256 уровней, 10-битное → 1024, 12-битное → 4096.
Форматы HDR используют 10–12 бит для расширенного диапазона, но в ML-датасетах чаще остаются 8-битные RGB/YCbCr из-за скорости и совместимости.

---

### 10. Метрики качества видео

При оценке сжатия и восстановления применяются:

1. **PSNR (Peak Signal-to-Noise Ratio):**
   [
   PSNR = 10 \log_{10}\frac{MAX_I^2}{MSE}.
   ]
   Измеряется в дБ, показывает уровень шума и потерь.
2. **SSIM (Structural Similarity Index):**
   [
   SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)},
   ]
   оценивает структурное сходство.
3. **VMAF (Video Multimethod Assessment Fusion)** — метрика Netflix, комбинирующая несколько показателей, близка к человеческому восприятию.

---

### 11. Артефакты сжатия и влияние на нейросети

Типичные искажения при сильной компрессии:

* **Blockiness** — видимые квадратные блоки;
* **Ringing** — ореолы вокруг контуров;
* **Blur** — потеря текстур;
* **Frame jumps** — нарушения временной когерентности при VFR.

Для сетей компьютерного зрения это критично: разрывы и шумы ухудшают детекцию границ, влияют на точность action recognition.
Поэтому рекомендуется:

* одинаковые параметры сжатия для train/test выборок;
* по возможности lossless-декодирование;
* фильтрация артефактов (например, `ffmpeg -vf hqdn3d`).

---

### 12. Пример кодирования в FFmpeg

```bash
ffmpeg -i input.mp4 -c:v libx264 -preset slow -crf 20 -g 12 -pix_fmt yuv420p output.mp4
```

Параметры:

* `-crf` — компромисс качество/битрейт (меньше = лучше);
* `-g` — длина GOP;
* `-preset` — скорость/качество кодирования;
* `-pix_fmt` — формат хранения пикселей (4:2:0, 4:4:4).

---

### 13. Архитектура декодирования и буферизация

Работа с видеопотоком в инженерных системах строится по определённой схеме. Видеофайл или поток — это не просто последовательность кадров, а совокупность закодированных пакетов данных, в которых видео, аудио и служебная информация идут параллельно. Чтобы получить отдельные кадры, система должна пройти несколько стадий:

```
Файл / Поток (MP4, MKV, TS, RTSP)
   ↓
Demuxer (демультиплексор)
   ↓
Parser (парсер NAL-единиц, GOP-структуры)
   ↓
Decoder (декодер кодека)
   ↓
Frame Buffer (буфер кадров)
   ↓
Препроцессинг → Модель → Визуализация / Запись
```

**Demuxer** отделяет видео- и аудиопотоки и передаёт их дальше по цепочке. Он работает асинхронно: пакеты приходят с разной скоростью и неравномерно.

**Parser** определяет, где начинаются и заканчиваются кадры (NAL units), восстанавливает последовательность GOP и подготавливает данные к декодированию.

**Decoder** — это основной вычислительный блок. Он превращает поток байтов, сжатых кодеком, обратно в кадры. Реализация может быть программной (CPU) или аппаратной (GPU, специализированный блок). Аппаратные декодеры, такие как **NVDEC** (NVIDIA), **VAAPI** (Intel) и **VideoToolbox** (macOS), значительно ускоряют процесс, поскольку работают параллельно с вычислениями модели.

**Frame Buffer** — кольцевой буфер (ring buffer), где хранится несколько последних кадров. Он обеспечивает согласование скоростей разных этапов: декодер может выдавать данные быстрее или медленнее, чем модель успевает их обрабатывать.

Для стабильного real-time воспроизведения важно контролировать **латентность (latency)** и **джиттер (jitter)**:

* *латентность* — средняя задержка между получением кадра и его показом;
* *джиттер* — колебание этой задержки.

Стабильная латентность особенно критична для **RTSP**-стримов и систем онлайн-видеонаблюдения: переполнение буфера приводит к остановке или отставанию видео, а слишком малый буфер — к разрывам.
